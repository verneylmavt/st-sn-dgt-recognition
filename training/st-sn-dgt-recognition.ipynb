{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code sets the random seed for reproducibility across random, NumPy, and PyTorch operations. This ensures consistent results by fixing the seed for both CPU and GPU computations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code handles the data preprocessing by applying transformations such as random rotation, affine transformations, and normalization for training data, while only applying normalization for test data. These transformations enhance generalization and standardize the pixel values.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code loads the MNIST dataset using the datasets library provided by torchvision.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = datasets.MNIST(root='../../data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, download=True, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code splits the training data into training and validation subsets, initializes data loaders for training, validation, and testing datasets, and specifies batch sizes for efficient data iteration.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 10000\n",
    "train_size = len(train_val_dataset) - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_iter = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code provides a function for visualizing images from the dataset with an option to save the displayed images. It allows filtering by label and displaying a specified number of images.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, label_to_display=None, num_images=1, save_images=False):\n",
    "    output_dir = \"../data/\"\n",
    "    if save_images:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 2))\n",
    "    count = 0\n",
    "    \n",
    "    for image, label in dataset:\n",
    "        if label_to_display is not None and label != label_to_display:\n",
    "            continue\n",
    "        \n",
    "        image = image.squeeze().numpy()\n",
    "        plt.subplot(1, num_images, count + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_images:\n",
    "            save_path = os.path.join(output_dir, f\"label_{label}_image_{count}.png\")\n",
    "            plt.imsave(save_path, image, cmap='gray')\n",
    "        \n",
    "        count += 1\n",
    "        if count >= num_images:\n",
    "            break\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images(train_dataset, label_to_display=0, num_images=1, save_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a Convolutional Neural Network (CNN) for digit classification, utilizing residual blocks for feature extraction and pooling layers for dimensionality reduction. The final layers include global average pooling, dropout for regularization, and a fully connected layer for classification.\n",
    "\n",
    "• Residual Block\n",
    "The inclusion of residual blocks ensures efficient training by allowing the network to learn identity mappings, addressing issues like vanishing gradients. This is particularly useful in deeper networks, where learning residual functions (the difference between the input and output) is easier than learning the full mapping directly. By incorporating these blocks, the model achieves better performance with fewer training epochs.\n",
    "\n",
    "• Layer (Layer 1 - Layer 6)\n",
    "- Each layer pair (e.g., layer1 + layer2, layer3 + layer4, etc.) extracts features at increasing levels of abstraction.\n",
    "- The initial layers capture basic patterns like edges and textures, while deeper layers focus on complex patterns, such as shapes and digit structures.\n",
    "- Residual blocks allow these layers to preserve essential information from earlier layers while refining feature representations.\n",
    "\n",
    "• Max Pooling\n",
    "The pooling layers (pool1, pool2, pool3) progressively downsample feature maps, reducing spatial dimensions while retaining critical information. This operation not only reduces computational complexity but also introduces translational invariance, which is essential for digit classification.\n",
    "\n",
    "• Global Average Pooling\n",
    "The adaptive average pooling layer aggregates spatial information into a single 1 × 1 feature map per channel. This approach reduces the feature map to a fixed size, independent of the input dimensions, making the architecture more versatile for inputs of varying sizes. By summarizing the feature maps, global average pooling reduces the risk of overfitting compared to fully connected layers with large weight matrices.\n",
    "\n",
    "• Fully Connected Layer\n",
    "The final linear layer maps the aggregated features to the output dimension (num_classes), which corresponds to the number of possible digit classes (e.g., 10 for digits 0–9).\n",
    "This layer produces logits, which can be converted into probabilities using a softmax function during evaluation or training.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # First Convolutional Layer for Residual Block\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        # Batch Normalization Layer for First Convolution\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Activation Layer for Non-Linear Transformation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Second Convolutional Layer for Residual Block\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        # Batch Normalization Layer for Second Convolution\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Downsampling Layer for Adjusting Identity Shortcut\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Identity Shortcut Connection\n",
    "        identity = x\n",
    "        \n",
    "        # First Convolutional Layer Transformation\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # Second Convolutional Layer Transformation\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Downsampling of Identity Shortcut (if Applicable)\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        # Addition of Identity Shortcut to Residual Block Output\n",
    "        out += identity\n",
    "        # Activation of Residual Block Output\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        # First Convolutional Layer for Feature Extraction\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # First Residual Block for Feature Refinement\n",
    "        self.layer2 = ResidualBlock(32, 32)\n",
    "        # First Max Pooling Layer for Spatial Downsampling\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Second Convolutional Layer for Feature Extraction\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Second Residual Block for Feature Refinement\n",
    "        self.layer4 = ResidualBlock(64, 64)\n",
    "        # Second Max Pooling Layer for Spatial Downsampling\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Third Convolutional Layer for Feature Extraction\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Third Residual Block for Feature Refinement\n",
    "        self.layer6 = ResidualBlock(128, 128)\n",
    "        # Third Max Pooling Layer for Spatial Downsampling\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling Layer for Spatial Dimension Reduction\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Dropout Layer for Regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # Fully Connected Layer for Digit Classification\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First Convolutional Layer Transformation\n",
    "        out = self.layer1(x)\n",
    "        # First Residual Block Transformation\n",
    "        out = self.layer2(out)\n",
    "        # First Max Pooling Layer Transformation\n",
    "        out = self.pool1(out)\n",
    "        \n",
    "        # Second Convolutional Layer Transformation\n",
    "        out = self.layer3(out)\n",
    "        # Second Residual Block Transformation\n",
    "        out = self.layer4(out)\n",
    "        # Second Max Pooling Layer Transformation\n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        # Third Convolutional Layer Transformation\n",
    "        out = self.layer5(out)\n",
    "        # Third Residual Block Transformation\n",
    "        out = self.layer6(out)\n",
    "        # Third Max Pooling Layer Transformation\n",
    "        out = self.pool3(out)\n",
    "        \n",
    "        # Global Average Pooling Transformation\n",
    "        out = self.global_avg_pool(out)\n",
    "        # Flattening of Pooled Output\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # Dropout Application to Flattened Output\n",
    "        out = self.dropout(out)\n",
    "        # Transformation of Flattened Features → Digit Scores\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to train the model for one epoch. It processes each batch, performs backpropagation, updates the model parameters, and calculates the training accuracy and loss.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, iter, criterion, optimizer):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in iter:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(iter.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines a function to evaluate the model on a validation set. It computes the loss and accuracy without updating the model parameters.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(net, iter, criterion):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in iter:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(iter.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code trains the model, monitors performance metrics, and visualizes the training and validation losses and accuracies over epochs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, train_iter, val_iter, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    num_epochs_used = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nepoch {epoch}/{num_epochs}\")\n",
    "        num_epochs_used += 1\n",
    "        train_loss, train_acc = train_epoch(net, train_iter, criterion, optimizer)\n",
    "        val_loss, val_acc = evaluate_epoch(net, val_iter, criterion)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f\"train loss: {train_loss:.4f}, train acc: {train_acc:.4f}, val loss: {val_loss:.4f}, val acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(net.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies, num_epochs_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, val_losses, val_accuracies, num_epochs_used = train_model(net, train_iter, val_iter, criterion, optimizer, scheduler, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, num_epochs_used + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(epochs_range, train_losses, label='train loss', linestyle='-', color='#2a7db8')\n",
    "plt.plot(epochs_range, train_accuracies, label='train acc', linestyle='--', color='green')\n",
    "plt.plot(epochs_range, val_losses, label='val loss', linestyle='-', color='red')\n",
    "plt.plot(epochs_range, val_accuracies, label='val acc', linestyle='--', color='magenta')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('value')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "In here, the code defines the cal_metrics function, which evaluates the trained model on the test dataset. It sets the model to evaluation mode, iterates through the test data loader, makes predictions, and accumulates the true and predicted labels. It then computes and prints the classification report (precision, recall, f1-score, support) for each digit.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_38440\\2640716579.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_metrics(net, test_iter):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_iter:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       980\n",
      "           1       1.00      1.00      1.00      1135\n",
      "           2       1.00      1.00      1.00      1032\n",
      "           3       1.00      1.00      1.00      1010\n",
      "           4       1.00      1.00      1.00       982\n",
      "           5       1.00      0.99      0.99       892\n",
      "           6       1.00      1.00      1.00       958\n",
      "           7       1.00      1.00      1.00      1028\n",
      "           8       1.00      1.00      1.00       974\n",
      "           9       1.00      1.00      1.00      1009\n",
      "\n",
      "    accuracy                           1.00     10000\n",
      "   macro avg       1.00      1.00      1.00     10000\n",
      "weighted avg       1.00      1.00      1.00     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal_metrics(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONNX Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net.to(device)\n",
    "net.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device=device)\n",
    "torch.onnx.export(net, dummy_input, \"model.onnx\", verbose=True,\n",
    "                    input_names=['input'], output_names=['output'], \n",
    "                    opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize_dynamic(\"model.onnx\", \"model-q.onnx\", weight_type=QuantType.QInt8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
